#!/usr/bin/env -S uv run
# /// script
# dependencies = ["requests", "simple-salesforce", "pandas", "python-dotenv"]
# ///
"""
Data Download & Conversion Pipeline

Salesforce ã‹ã‚‰CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€NDJSONå½¢å¼ã«å¤‰æ›ã—ã¦ workspace/ ã«é…ç½®ã™ã‚‹ã€‚

Usage:
    uv run download.py
"""

import json
import os
import sys
import re
from pathlib import Path
from datetime import datetime, timedelta
import requests
from simple_salesforce import Salesforce
from urllib.parse import urlparse
import pandas as pd
from dotenv import load_dotenv


def get_report_ids() -> dict:
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ¬ãƒãƒ¼ãƒˆIDã‚’å–å¾—"""
    report_ids_str = os.getenv("SALESFORCE_REPORT_IDS")

    if not report_ids_str:
        print("âŒ Error: SALESFORCE_REPORT_IDS ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   .env ãƒ•ã‚¡ã‚¤ãƒ«ã«ä»¥ä¸‹ã®å½¢å¼ã§è¿½åŠ ã—ã¦ãã ã•ã„:")
        print(
            "   SALESFORCE_REPORT_IDS='ãƒ¬ãƒãƒ¼ãƒˆID1:ãƒ•ã‚¡ã‚¤ãƒ«å1.csv,ãƒ¬ãƒãƒ¼ãƒˆID2:ãƒ•ã‚¡ã‚¤ãƒ«å2.csv,...'"
        )
        sys.exit(1)

    try:
        # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ãƒ‘ãƒ¼ã‚¹: "ID1:file1.csv,ID2:file2.csv,..."
        report_ids = {}
        for pair in report_ids_str.split(","):
            if ":" in pair:
                report_id, filename = pair.split(":", 1)
                report_ids[report_id.strip()] = filename.strip()

        if not report_ids:
            print("âŒ Error: SALESFORCE_REPORT_IDS ãŒç©ºã§ã™")
            print("   .env ãƒ•ã‚¡ã‚¤ãƒ«ã§æ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
            sys.exit(1)

        return report_ids
    except Exception as e:
        print(f"âŒ Error: ãƒ¬ãƒãƒ¼ãƒˆIDã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        print("   .env ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        print("   æ­£ã—ã„å½¢å¼: SALESFORCE_REPORT_IDS='ID1:file1.csv,ID2:file2.csv'")
        sys.exit(1)


def parse_yaml_like_credentials(creds_str: str) -> dict:
    """YAMLé¢¨ã®èªè¨¼æƒ…å ±æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹"""
    # {key: value, ...} å½¢å¼ã‚’ {"key": "value", ...} å½¢å¼ã«å¤‰æ›
    # ã‚¯ã‚©ãƒ¼ãƒˆãªã—ã®ã‚­ãƒ¼ã¨å€¤ã‚’ã‚¯ã‚©ãƒ¼ãƒˆä»˜ãã«å¤‰æ›
    creds_str = creds_str.strip()

    # ã™ã§ã«JSONå½¢å¼ãªã‚‰ãã®ã¾ã¾
    if creds_str.startswith('{"'):
        return json.loads(creds_str)

    # YAMLé¢¨å½¢å¼ã®å ´åˆã€æ‰‹å‹•ã§ãƒ‘ãƒ¼ã‚¹
    result = {}
    # {key: value, key2: value2} ã‚’æŠ½å‡º
    content = creds_str.strip("{}")
    pairs = content.split(",")

    for pair in pairs:
        if ":" in pair:
            key, value = pair.split(":", 1)
            key = key.strip().strip('"').strip("'")
            value = value.strip().strip('"').strip("'")
            result[key] = value

    return result


def get_credentials() -> dict:
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—"""
    creds_str = os.getenv("SALESFORCE_CREDENTIALS")
    if not creds_str:
        print("âŒ Error: SALESFORCE_CREDENTIALS ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)

    try:
        return parse_yaml_like_credentials(creds_str)
    except Exception as e:
        print(f"âŒ Error: èªè¨¼æƒ…å ±ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


def download_report(
    session_id: str, instance_url: str, report_id: str, output_path: Path
) -> bool:
    """ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆURLï¼ˆClassic UIæ–¹å¼ï¼‰
    export_url = f"{instance_url}/{report_id}?export=1&enc=UTF-8&xf=csv&isdtp=p1"

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæº–å‚™
    session = requests.Session()
    parsed_url = urlparse(instance_url)
    session.cookies.set("sid", session_id, domain=parsed_url.hostname)

    headers = {
        "Authorization": f"Bearer {session_id}",
        "Accept": "text/csv",
        "User-Agent": "Mozilla/5.0 (compatible; SalesforceReportExporter/1.0)",
    }

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
    print(f"  ğŸ“¡ {output_path.name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    response = session.get(export_url, headers=headers, timeout=60)

    if response.status_code != 200:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}")
        return False

    # CSVä¿å­˜
    output_path.write_text(response.text, encoding="utf-8")
    print(f"  âœ… ä¿å­˜å®Œäº†: {output_path.name}")
    return True


def filter_candidates(
    df, recent_interview_days=60, min_survey_year=2024, valid_ranks=["S", "A", "B"]
):
    """æ±‚è·è€…ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    original_count = len(df)
    print(f"ğŸ” æ±‚è·è€…ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­... (å…ƒ: {original_count}ä»¶)")

    # ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–ï¼ˆå‰å¾Œã®ç©ºç™½ãƒ»BOMé™¤å»ï¼‰
    df.columns = df.columns.str.strip()

    # ãƒ©ãƒ³ã‚¯ãƒ•ã‚£ãƒ«ã‚¿
    rank_col = "å€‹äººãƒ¦ãƒ¼ã‚¶ãƒ¼/ä¼æ¥­: ç™»éŒ²æ™‚ãƒ©ãƒ³ã‚¯"
    if rank_col in df.columns:
        before = len(df)
        df = df[df[rank_col].isin(valid_ranks)].copy()
        print(
            f"  ãƒ©ãƒ³ã‚¯ãƒ•ã‚£ãƒ«ã‚¿ ({', '.join(valid_ranks)}): {len(df)}ä»¶ ({before - len(df)}ä»¶é™¤å¤–)"
        )

    # ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”æ—¥ãƒ•ã‚£ãƒ«ã‚¿
    survey_col = "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”æ—¥æ™‚"
    if survey_col in df.columns:
        min_survey_date = datetime(min_survey_year, 1, 1)
        df.loc[:, survey_col] = pd.to_datetime(df[survey_col], errors="coerce")
        before = len(df)
        df = df[df[survey_col] >= min_survey_date].copy()
        print(
            f"  ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆæ—¥ãƒ•ã‚£ãƒ«ã‚¿ ({min_survey_year}å¹´ä»¥é™): {len(df)}ä»¶ ({before - len(df)}ä»¶é™¤å¤–)"
        )

    # åˆå›é¢è«‡æ—¥ OR é¸è€ƒä¸­ãƒ•ã‚£ãƒ«ã‚¿
    interview_col = "å€‹äººãƒ¦ãƒ¼ã‚¶ãƒ¼/ä¼æ¥­: åˆå›é¢è«‡æ—¥æ™‚"
    status_col = "é¸è€ƒã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"

    if interview_col in df.columns:
        recent_date = datetime.now() - timedelta(days=recent_interview_days)
        df.loc[:, interview_col] = pd.to_datetime(df[interview_col], errors="coerce")

        active_statuses = [
            "æ›¸é¡é¸è€ƒä¸­",
            "ä¸€æ¬¡é¢æ¥ä¸­",
            "äºŒæ¬¡é¢æ¥ä¸­",
            "æœ€çµ‚é¢æ¥ä¸­",
            "ã‚ªãƒ•ã‚¡ãƒ¼é¢è«‡ä¸­",
        ]
        recent_interview = df[interview_col] >= recent_date

        if status_col in df.columns:
            active_selection = df[status_col].isin(active_statuses)
            before = len(df)
            df = df[recent_interview | active_selection].copy()
            print(
                f"  é¢è«‡æ—¥({recent_interview_days}æ—¥ä»¥å†…) OR é¸è€ƒä¸­: {len(df)}ä»¶ ({before - len(df)}ä»¶é™¤å¤–)"
            )

    # æœ€çµ‚æ›´æ–°æ—¥ãƒ•ã‚£ãƒ«ã‚¿
    update_col = "æœ€çµ‚æ›´æ–°æ—¥"
    if update_col in df.columns:
        last_year = datetime.now() - timedelta(days=365)
        df.loc[:, update_col] = pd.to_datetime(df[update_col], errors="coerce")
        before = len(df)
        df = df[df[update_col] >= last_year].copy()
        print(f"  æœ€çµ‚æ›´æ–°æ—¥(365æ—¥ä»¥å†…): {len(df)}ä»¶ ({before - len(df)}ä»¶é™¤å¤–)")

    print(f"âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿: {original_count}ä»¶ â†’ {len(df)}ä»¶\n")
    return df


def filter_jobs(df, job_status="ã‚¢ã‚¯ãƒ†ã‚£ãƒ–"):
    """æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    original_count = len(df)
    print(f"ğŸ” æ±‚äººãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­... (å…ƒ: {original_count}ä»¶)")

    # ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–
    df.columns = df.columns.str.strip()

    # æ±‚äººçŠ¶æ…‹ãƒ•ã‚£ãƒ«ã‚¿
    status_col = None
    if "æ±‚äººçŠ¶æ…‹" in df.columns:
        status_col = "æ±‚äººçŠ¶æ…‹"
    elif "æ±‚äººç¥¨ æ±‚äººçŠ¶æ…‹" in df.columns:
        status_col = "æ±‚äººç¥¨ æ±‚äººçŠ¶æ…‹"

    if status_col:
        before = len(df)
        df = df[df[status_col] == job_status]
        print(
            f"  æ±‚äººçŠ¶æ…‹ãƒ•ã‚£ãƒ«ã‚¿ ({job_status}): {len(df)}ä»¶ ({before - len(df)}ä»¶é™¤å¤–)"
        )

    print(f"âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿: {original_count}ä»¶ â†’ {len(df)}ä»¶\n")
    return df


def to_ndjson(df, output_path):
    """DataFrameã‚’NDJSONå½¢å¼ã§ä¿å­˜"""
    print(f"ğŸ’¾ NDJSONä¿å­˜ä¸­: {output_path}")

    with open(output_path, "w", encoding="utf-8") as f:
        for _, row in df.iterrows():
            record = {}
            for key, value in row.items():
                if pd.isna(value):
                    record[key] = None
                elif isinstance(value, (pd.Timestamp, datetime)):
                    record[key] = str(value)
                else:
                    record[key] = value

            f.write(json.dumps(record, ensure_ascii=False) + "\n")

    print(f"âœ… ä¿å­˜å®Œäº†: {len(df)}ä»¶\n")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    project_root = Path(__file__).parent.parent
    tmp_dir = project_root / "tmp"
    workspace_dir = project_root / "workspace"

    # .env ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ (.envãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚€ã€‚æ—¢å­˜ã®ç’°å¢ƒå¤‰æ•°ã¯ä¸Šæ›¸ãã—ãªã„)
    load_dotenv(dotenv_path=project_root / ".env", override=False)

    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    tmp_dir.mkdir(exist_ok=True)
    workspace_dir.mkdir(exist_ok=True)

    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
    recent_interview_days = int(os.environ.get("RECENT_INTERVIEW_DAYS", "60"))
    min_survey_year = int(os.environ.get("MIN_SURVEY_YEAR", "2024"))
    valid_ranks = os.environ.get("VALID_RANKS", "S,A,B").split(",")
    job_status = os.environ.get("JOB_STATUS", "ã‚¢ã‚¯ãƒ†ã‚£ãƒ–")

    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ“¥ Data Download & Conversion Pipeline")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print()

    # Step 1: Salesforce ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    print("ğŸ” Step 1: Salesforce ã«æ¥ç¶šä¸­...")
    creds = get_credentials()
    sf = Salesforce(
        username=creds["username"],
        password=creds["password"],
        security_token=creds["security_token"],
        domain=creds.get("domain", "login"),
    )

    session_id = sf.session_id
    instance_url = sf.sf_instance

    if not instance_url.startswith("http"):
        instance_url = f"https://{instance_url}"

    print(f"âœ… æ¥ç¶šæˆåŠŸ: {instance_url}")
    print()

    # ãƒ¬ãƒãƒ¼ãƒˆIDå–å¾—
    report_ids = get_report_ids()

    print(f"ğŸ“¥ Step 2: ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹ ({len(report_ids)}ä»¶)")
    print()

    success_count = 0
    for report_id, filename in report_ids.items():
        output_path = tmp_dir / filename
        if download_report(session_id, instance_url, report_id, output_path):
            success_count += 1

    print()
    if success_count < len(report_ids):
        print(f"âš ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {success_count}/{len(report_ids)}ä»¶")
        sys.exit(1)

    print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {success_count}/{len(report_ids)}ä»¶")
    print()

    # Step 3: NDJSON å¤‰æ›
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ”„ Step 3: NDJSON å¤‰æ›")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print()
    print(f"ğŸ“‹ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š:")
    print(f"  - åˆå›é¢è«‡æ—¥: {recent_interview_days}æ—¥ä»¥å†…")
    print(f"  - ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”: {min_survey_year}å¹´ä»¥é™")
    print(f"  - ç™»éŒ²æ™‚ãƒ©ãƒ³ã‚¯: {', '.join(valid_ranks)}")
    print(f"  - æ±‚äººçŠ¶æ…‹: {job_status}")
    print()

    # æ±‚è·è€…å‡¦ç†
    candidates_csv = tmp_dir / "æ±‚è·è€….csv"
    print("ğŸ“– æ±‚è·è€…RAWãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    candidates_df = pd.read_csv(candidates_csv, encoding="utf-8-sig")
    candidates_df = filter_candidates(
        candidates_df,
        recent_interview_days=recent_interview_days,
        min_survey_year=min_survey_year,
        valid_ranks=valid_ranks,
    )
    to_ndjson(candidates_df, workspace_dir / "candidates.ndjson")

    # æ±‚äººå‡¦ç†
    jobs_csv = tmp_dir / "æ±‚äººç¥¨.csv"
    print("ğŸ“– æ±‚äººRAWãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    jobs_df = pd.read_csv(jobs_csv, encoding="utf-8-sig")
    jobs_df = filter_jobs(jobs_df, job_status=job_status)
    to_ndjson(jobs_df, workspace_dir / "jobs.ndjson")

    # ä¼æ¥­å‡¦ç†ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰
    companies_csv = tmp_dir / "ä¼æ¥­.csv"
    print("ğŸ“– ä¼æ¥­RAWãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    companies_df = pd.read_csv(companies_csv, encoding="utf-8-sig")
    companies_df.columns = companies_df.columns.str.strip()
    print(f"âœ… èª­ã¿è¾¼ã¿å®Œäº†: {len(companies_df)}ä»¶")
    print()
    to_ndjson(companies_df, workspace_dir / "companies.ndjson")

    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("âœ… å…¨ã¦å®Œäº†ï¼")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print()
    print("ğŸ“‚ å‡ºåŠ›å…ˆ:")
    print(f"  - {workspace_dir / 'candidates.ndjson'} ({len(candidates_df)}ä»¶)")
    print(f"  - {workspace_dir / 'jobs.ndjson'} ({len(jobs_df)}ä»¶)")
    print(f"  - {workspace_dir / 'companies.ndjson'} ({len(companies_df)}ä»¶)")


if __name__ == "__main__":
    main()
